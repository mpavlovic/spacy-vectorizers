{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn compatible vectorizers built with spaCy NLP famework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will show you basic examples of how and when to use customized classes and vectorizers inspired by ```scikit-learn```'s ```CountVectorizer```, which add more accurate tokenization and lemmatization funcitonality with the help of <a href='https://spacy.io/'>spaCy</a> NLP framework. Simple <a href='https://keras.io/preprocessing/text/'>Keras</a>-like punctuation removal support is also added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the imports first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from vectorizers import SpacyTokenizer\n",
    "from vectorizers import SpacyLemmatizer\n",
    "from vectorizers import SpacyPipeProcessor\n",
    "from vectorizers import SpacyTokenCountVectorizer\n",
    "from vectorizers import SpacyLemmaCountVectorizer\n",
    "from vectorizers import SpacyWord2VecVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will load ```en_core_web_md``` model for spaCy and create some example single-sentence documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example documents\n",
    "raw_documents = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "                 \"This is a test sentence.\",\n",
    "                 \"This sentence contains exclamation mark, comma and (round brackets)!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the helper classes for tokenization and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpacyTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SpacyTokenizer``` uses spaCy <a href='https://spacy.io/usage/linguistic-features#section-tokenization'>tokenizer</a> for document tokenization. When ```join_str``` argument is set to ```None```, the result will be a ```list``` of lists of strings (tokens). Punctuation from the ```ignore_chars``` argument will be removed in every separate token, but empty tokens will be kept. You can also specify ```batch_size``` and ```n_threads``` arguments for parallel processing of large datasets. Lowercasing isn't performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '']\n",
      "['This', 'is', 'a', 'test', 'sentence', '']\n",
      "['This', 'sentence', 'contains', 'exclamation', 'mark', '', 'comma', 'and', '', 'round', 'brackets', '', '']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SpacyTokenizer(nlp, join_str=None, ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', \n",
    "                           batch_size=10000, n_threads=1)\n",
    "\n",
    "tokens = tokenizer(raw_documents) # generator object is returned\n",
    "for tokenized_doc in tokens:\n",
    "    print(tokenized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the difference when ```join_str``` is set to space cahracter. SpacyTokenizer will return the ```list``` of strings which are joined tokens (together with empty punctuation-only tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog \n",
      "This is a test sentence \n",
      "This sentence contains exclamation mark  comma and  round brackets  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = SpacyTokenizer(nlp, join_str=' ', n_threads=1)\n",
    "tokens = tokenizer(raw_documents) # generator object is returned\n",
    "for tokenized_doc in tokens:\n",
    "    print(tokenized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this example shows a usual result from tokenization and punctuation removal. Notice that you must call the ```split()``` method to obtain a list of tokens without empty ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "['This', 'is', 'a', 'test', 'sentence']\n",
      "['This', 'sentence', 'contains', 'exclamation', 'mark', 'comma', 'and', 'round', 'brackets']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SpacyTokenizer(nlp, join_str=' ', n_threads=1)\n",
    "tokens = tokenizer(raw_documents) # generator object is returned\n",
    "for tokenized_doc in tokens:\n",
    "    print(tokenized_doc.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpacyLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SpacyLemmatizer``` is very similar to ```SpacyTokenizer```, but it returns lowercased lemmas instead of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '']\n",
      "['this', 'be', 'a', 'test', 'sentence', '']\n",
      "['this', 'sentence', 'contain', 'exclamation', 'mark', '', 'comma', 'and', '', 'round', 'bracket', '', '']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = SpacyLemmatizer(nlp, join_str=None, ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~', \n",
    "                             batch_size=10000, n_threads=1)\n",
    "lemmas = lemmatizer(raw_documents) # generator object is returned\n",
    "for lemmatized_doc in lemmas:\n",
    "    print(lemmatized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jump over the lazy dog \n",
      "this be a test sentence \n",
      "this sentence contain exclamation mark  comma and  round bracket  \n"
     ]
    }
   ],
   "source": [
    "lemmatizer = SpacyLemmatizer(nlp, join_str=' ', n_threads=1)\n",
    "lemmas = lemmatizer(raw_documents) # generator object is returned\n",
    "for lemmatized_doc in lemmas:\n",
    "    print(lemmatized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog']\n",
      "['this', 'be', 'a', 'test', 'sentence']\n",
      "['this', 'sentence', 'contain', 'exclamation', 'mark', 'comma', 'and', 'round', 'bracket']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = SpacyLemmatizer(nlp, join_str=' ', n_threads=1)\n",
    "lemmas = lemmatizer(raw_documents) # generator object is returned\n",
    "for lemmatized_doc in lemmas:\n",
    "    print(lemmatized_doc.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpacyTokenCountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SpacyTokenCountVectorizer``` inherits ```scikit-learn```'s ```CountVectorizer``` to enable tokenization from ```spaCy``` models. Its ```fit()```, ```fit_transform()``` and ```transform()``` methods accept iterable of <a href=https://spacy.io/api/doc>Doc</a> objects as ```spacy_docs``` (```X``` in ```scikit-learn```) parameter. This iterable can be obtained from ```SpacyPipeProcessor``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp = SpacyPipeProcessor(nlp, n_threads=1)  # creates iterable of spaCy Doc objects\n",
    "spacy_docs = spp(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we can see that the result of ```SpacyTokenCountVectorizer```'s ```fit_transform()``` method is a CSR sparse matrix, just like a standard CountVectorizer would return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stcv = SpacyTokenCountVectorizer(ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "count_vectors = stcv.fit_transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 18, 'this': 19, 'mark': 12, 'quick': 14, 'brackets': 2, 'dog': 6, 'a': 0, 'round': 15, 'jumps': 10, 'brown': 3, 'over': 13, 'fox': 8, 'lazy': 11, 'comma': 4, 'sentence': 16, 'contains': 5, 'exclamation': 7, 'is': 9, 'and': 1, 'test': 17}\n"
     ]
    }
   ],
   "source": [
    "print(stcv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialize a ```SpacyPipeProcessor``` object with the ```multi_iters``` parameter set to ```True```, the result of its ```__call__``` method will be a list of ```Doc``` objects, instead of a single ```generator```. This allows you to iterate multiple times thorugh returned objects if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "spacy_docs = spp(raw_documents)\n",
    "\n",
    "stcv = SpacyTokenCountVectorizer(ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "stcv.fit(spacy_docs)\n",
    "count_vectors = stcv.transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 18, 'this': 19, 'mark': 12, 'quick': 14, 'brackets': 2, 'dog': 6, 'a': 0, 'round': 15, 'jumps': 10, 'brown': 3, 'over': 13, 'fox': 8, 'lazy': 11, 'comma': 4, 'sentence': 16, 'contains': 5, 'exclamation': 7, 'is': 9, 'and': 1, 'test': 17}\n"
     ]
    }
   ],
   "source": [
    "print(stcv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpacyLemmaCountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SpacyLemmaCountVectorizer``` is very similar to ```SpacyTokenCountVectorizer```, but it performs lemmatization instead of tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp = SpacyPipeProcessor(nlp, n_threads=1)\n",
    "spacy_docs = spp(raw_documents);\n",
    "\n",
    "slcv = SpacyLemmaCountVectorizer(ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "count_vectors = slcv.fit_transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 7, 'contain': 6, 'sentence': 16, 'the': 18, 'this': 19, 'round': 15, 'brown': 4, 'exclamation': 8, 'fox': 9, 'mark': 12, 'jump': 10, 'over': 13, 'be': 2, 'and': 1, 'a': 0, 'comma': 5, 'test': 17, 'quick': 14, 'lazy': 11, 'bracket': 3}\n"
     ]
    }
   ],
   "source": [
    "print(slcv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp = SpacyPipeProcessor(nlp, n_threads=1, multi_iters=True)\n",
    "spacy_docs = spp(raw_documents);\n",
    "\n",
    "slcv = SpacyLemmaCountVectorizer(ignore_chars='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "slcv.fit(spacy_docs)\n",
    "count_vectors = slcv.transform(spacy_docs); count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 7, 'contain': 6, 'sentence': 16, 'the': 18, 'this': 19, 'round': 15, 'brown': 4, 'exclamation': 8, 'fox': 9, 'mark': 12, 'jump': 10, 'over': 13, 'be': 2, 'and': 1, 'a': 0, 'comma': 5, 'test': 17, 'quick': 14, 'lazy': 11, 'bracket': 3}\n"
     ]
    }
   ],
   "source": [
    "print(slcv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll test classes described above, in the modified Olivier Grisel's example from <a href=\"http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\">here</a>. Instead of ```LogisticRegression``` in the original example, we'll use ```LinearSVC```. This code samples show a grid search over several parameters in a text processing ```Pipeline``` on the 2 categories of 20 newsgroup dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc']\n",
      "857 documents\n",
      "2 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "random_state = 42 \n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc']\n",
      "857 documents\n",
      "2 categories\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 84.489s\n",
      "\n",
      "Best score: 0.943\n",
      "Best parameters set:\n",
      "\tclf__C: 100\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(random_state=random_state))\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score using ```CountVectorizer``` was 94,3%. Now we will create ```spacy_docs``` list for customized vectorizers and perform grid searches using ```SpacyTokenCountVectorizer``` and ```SpacyLemmaCountVectorizer```. Running time of theirs methods is much longer when compared to ```CountVectorizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset with spaCy...\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Processing dataset with spaCy...')\n",
    "spacy_docs = SpacyPipeProcessor(nlp, multi_iters=True, n_threads=1)(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset with Spacy...\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed: 32.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1966.337s\n",
      "\n",
      "Best score: 0.940\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', SpacyTokenCountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(random_state=random_state))\n",
    "])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "    \n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(spacy_docs, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ```SpacyTokenCountVectorizer``` we obtained 94% with different best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset with Spacy...\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed: 33.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2013.056s\n",
      "\n",
      "Best score: 0.935\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', SpacyLemmaCountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(random_state=random_state))\n",
    "])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(spacy_docs, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93,5% was the best result for ```SpacyLemmaCountVectorizer```. It seems like these custom vectorizers aren't a very good choice for concrete dataset, and a more extesive hyperparameter search and preprocessing is probably needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpacyWord2VecVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SpacyWord2VecVectorizer``` converts a ```list``` of ```Doc``` objects to their vector representations. Vectors are stored in a ```float32``` ```numpy``` array, where the number of rows equals to the number of documents and the number of columns is a vector dimensionality, which depends on the ```nlp``` model used. Word vectors have 300 dimensions in this case. When the ```sparsify``` parameter is ```True```, the resulting matrix will be sparse (CSR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:*** ```SpacWord2VecVectorizer``` is **not thread safe** at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x300 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp = SpacyPipeProcessor(nlp, n_threads=1)\n",
    "spacy_docs = spp(raw_documents)\n",
    "\n",
    "w2v = SpacyWord2VecVectorizer(sparsify=True)\n",
    "word_vectors = w2v.fit_transform(spacy_docs); word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use ```fit()``` and ```transform()``` methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x300 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp = SpacyPipeProcessor(nlp, n_threads=1)\n",
    "spacy_docs = spp(raw_documents)\n",
    "\n",
    "w2v = SpacyWord2VecVectorizer(sparsify=True)\n",
    "word_vectors = w2v.fit(spacy_docs).transform(spacy_docs); word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a classification test with ```SpacyWord2VecVectorizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset with Spacy...\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000)}\n",
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    7.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.457s\n",
      "\n",
      "Best score: 0.842\n",
      "Best parameters set:\n",
      "\tclf__C: 100\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', SpacyWord2VecVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC(random_state=random_state))\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': (0.001, 0.01, 0.1, 1, 10, 100, 1000),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, verbose=1) #  n_jobs=1 for thread safety\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(spacy_docs, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84,2% suggests that a larger hyperparameter search space is needed, together with other featrues such as bag of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 573 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = CountVectorizer().fit(data.data).transform(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = SpacyTokenCountVectorizer().fit(spacy_docs).transform(spacy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = SpacyLemmaCountVectorizer().fit(spacy_docs).transform(spacy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features = SpacyWord2VecVectorizer().fit(spacy_docs).transform(spacy_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we see that custom vectorizers are about 4 times slower than original ```CountVectorizer```. This shows that their tokenizers and lemmatizers should be used as a preprocessing step before extensive hyperparameter optimization. As tihs <a href=\"https://stackoverflow.com/a/45212615\">answer</a> suggests, ```CountVectorizer``` can be nicely used for vectorization of pre-tokenized or pre-lemmatized documents, since it's a faster and more memory friendly solution. Moreover, customized vectorizers didn't show performance imporovement on the small subset of 20 newsgroups dataset used here, but this isn't an evidence that they are not useful in general."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
